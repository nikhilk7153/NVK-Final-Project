{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8aWRCoSP2W4c",
        "outputId": "0bb31053-e62e-4e9c-9d2d-0ef6b5380a50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.9.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.8/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (110 kB)\n",
            "\u001b[K     |████████████████████████████████| 110 kB 54.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.73 pyahocorasick-1.4.4 textsearch-0.0.24\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 4.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 82.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 59.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!pip install vaderSentiment\n",
        "!pip install contractions\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1rTDIGWc2W4h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8fd0ab-e2f2-4d5f-ee4b-f1bad98dcbc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "import spacy\n",
        "import nltk\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "nltk.download('punkt')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hNTo1Ws32W4k"
      },
      "outputs": [],
      "source": [
        "# RAKE Implementation\n",
        "import nltk\n",
        "import string\n",
        "import numpy\n",
        "import sys\n",
        "numpy.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "# https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents\n",
        "def RakeExtract(text):\n",
        "        text_tokens = nltk.word_tokenize(text)\n",
        "        sub_tokens = []\n",
        "\n",
        "        # delimiting tokenized sentence based on stop words\n",
        "        for word in text_tokens:\n",
        "            if word in STOP_WORDS or word in string.punctuation:\n",
        "                sub_tokens.append(\"-\")\n",
        "            else:\n",
        "                sub_tokens.append(word)\n",
        "\n",
        "\n",
        "        spacetokens = []\n",
        "\n",
        "        res = \"\"\n",
        "        \n",
        "        # more delimiation edge case handling\n",
        "        # joining words together into phrases that are adjacent to each other in delimited sentence\n",
        "        for i in range(0,len(sub_tokens)-1):\n",
        "            if sub_tokens[i] == \"-\":\n",
        "                if res != \"\":\n",
        "                    spacetokens.append(res)\n",
        "                spacetokens.append(\"-\")\n",
        "                res = \"\"\n",
        "            if res == \"\" and sub_tokens[i] != \"-\":\n",
        "                res = sub_tokens[i]\n",
        "\n",
        "            if sub_tokens[i] != \"-\" and sub_tokens[i+1] != \"-\":\n",
        "                res+= \" \" + sub_tokens[i+1]\n",
        "\n",
        "        if sub_tokens[-1] != \"-\":\n",
        "            res += sub_tokens[-1]\n",
        "        spacetokens.append(res)\n",
        "\n",
        "        # Keeping track of count of a specific word in wrdcnt dictionary\n",
        "        wrdcnt = {}\n",
        "        for i in sub_tokens:\n",
        "            if i!= \"-\":\n",
        "                wrdcnt[i] = wrdcnt.get(i,0)+1\n",
        "\n",
        "\n",
        "        # This function initializes the vocabulary of the text\n",
        "        # Returns a word dictionary initalized to a unique index for every word\n",
        "        # This dictionary will store the index of the word in the 2-D Graph Adjacency List\n",
        "        def getinitinfo(tokens):\n",
        "\n",
        "            wordset = set()\n",
        "            worddict = {}\n",
        "            for i in tokens:\n",
        "                if i != \"-\":\n",
        "                    wordset.add(i)\n",
        "\n",
        "            cnt = 0\n",
        "            for i in wordset:\n",
        "                worddict[i] = cnt\n",
        "                cnt+=1\n",
        "            return worddict\n",
        "\n",
        "        # create graph to store counts of coocurrences of words within the same phrase in text\n",
        "        dic = getinitinfo(sub_tokens)\n",
        "        N = len(dic)\n",
        "        graph = np.zeros((N,N))\n",
        "\n",
        "        # Update graph with the adjacent words that appear together \n",
        "        for i in spacetokens:\n",
        "            if i!= \"-\":\n",
        "                spl = i.split()\n",
        "                for i in range(len(spl)):\n",
        "                    for j in range(i, len(spl)):\n",
        "                        graph[dic[spl[i]]][dic[spl[j]]] +=1\n",
        "                        if i!=j:\n",
        "                            graph[dic[spl[j]]][dic[spl[i]]] +=1\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # calculate Degree of word node in graph and the count of word in text\n",
        "        # divide degree/wrdcnt to get imortance score for each word\n",
        "        # caclulate important phrases score by adding score for each word in phrase\n",
        "        degree = np.sum(graph, axis=0)\n",
        "        for i, (k,v) in enumerate(dic.items()):\n",
        "\n",
        "            results.append((k,degree[v]/wrdcnt[k]))\n",
        "\n",
        "        results.sort(key=lambda x:x[1])\n",
        "        results = results[::-1]\n",
        "\n",
        "        keywords = set()\n",
        "        for word in spacetokens:\n",
        "            texspl = word.split()\n",
        "            score = 0\n",
        "            for x in results:\n",
        "                for y in texspl:\n",
        "                    if x[0] == y:\n",
        "                        score += x[1] \n",
        "            keywords.add((word,score))\n",
        "\n",
        "        # sort phrases by summation of their words score in increasing order\n",
        "        keywords = list(keywords)\n",
        "        keywords.sort(key=lambda x:x[1])\n",
        "        keywords = keywords[::-1]\n",
        "        keywords = keywords[:len(keywords)//3]\n",
        "\n",
        "        return keywords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1tv7ZNiw2W4l"
      },
      "outputs": [],
      "source": [
        "import contractions\n",
        "\n",
        "text = '''\n",
        "Poor customer experience and treatment received by banker Connie Walksler. I was expecting a warm, professional environment and it was definitely the contrary. This specific banker needs more training on wire transfers to Mexico especially, the lack of knowledge made me feel unsure about my transaction and business. She eventually helped me with this transfer but I left feeling not confident and uncomfortable.\n",
        "'''\n",
        "\n",
        "expanded_words = []   \n",
        "for word in text.split():\n",
        "  # using contractions.fix to expand the shortened words\n",
        "  expanded_words.append(contractions.fix(word))  \n",
        "   \n",
        "expanded_text = ' '.join(expanded_words)\n",
        "expanded_text = expanded_text.lower()\n",
        "expanded_text = expanded_text.replace(',', '')\n",
        "\n",
        "vals = RakeExtract(expanded_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "for extract in vals:\n",
        "          sid_obj = SentimentIntensityAnalyzer()\n",
        "          sentiment_dict = sid_obj.polarity_scores(extract[0])\n",
        "          \n",
        "          if sentiment_dict['compound'] >= 0.10 :\n",
        "            predicted_sentiments.append((extract , 'positive'))\n",
        " \n",
        "          elif sentiment_dict['compound'] <= - 0.10 :\n",
        "            predicted_sentiments.append((extract , 'negative'))\n",
        "          else :\n",
        "            predicted_sentiments.append((extract , 'neutral'))\n",
        "\n",
        "predicted_sentiments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mdu-pFipcqDB",
        "outputId": "744b4a18-74d1-4559-dce3-2410a7d95b25"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('poor customer experience', 9.0), 'negative'),\n",
              " (('banker connie walksler', 9.0), 'neutral'),\n",
              " (('specific banker needs', 9.0), 'neutral'),\n",
              " (('warm professional environment', 9.0), 'positive'),\n",
              " (('feel unsure', 4.0), 'negative'),\n",
              " (('treatment received', 4.0), 'neutral'),\n",
              " (('left feeling', 4.0), 'positive'),\n",
              " (('poor customer experience', 9.0), 'negative'),\n",
              " (('banker connie walksler', 9.0), 'neutral'),\n",
              " (('specific banker needs', 9.0), 'neutral'),\n",
              " (('warm professional environment', 9.0), 'positive'),\n",
              " (('feel unsure', 4.0), 'negative'),\n",
              " (('treatment received', 4.0), 'neutral'),\n",
              " (('left feeling', 4.0), 'positive')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}